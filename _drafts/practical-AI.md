---
layout: post
title: Practical AI
---

It is well known that artificial intelligence (AI) has gotten through multiple tides and ?? through its history. People's assumption about how quickly implement human level intelligence has been strikingly wrong (including Alan Turing's prediction of the necessary memory to replicate human intelligence in 1950). In spite of the recent successful demonstration of AlphaGo and various deep learning techniques, the progress toward human-level intelligence seems to be slower compared to all the hypes around artificial intelligence.

At this current stage, I believe that we have to focus on is practical artificial intelligence. 

One of the definitions of intelligence, I think that the following one from ??, "". I believe that the definition includes the pragmatism. In order to achieve the pragmatism, I believe that the following hurdles should be overcome.

1. Expensive Data

Though some of mechanisms (e.g., reinforcement learning) have been developed to circumvent the cost of data collection, cleansing, and annotation, genernally many researchers and engineers are stuck in this issue. Crowdsourcing could be another way to distribute the arduous burden to multiple people and several approaches have been divsed (e.g., divice and conqor [??]), but often it has limitations in achieving the highest quality. Trained data curators are often necessary, and retaining skilled data curators is often problematic because of the innate tediousness of job.

Facebook M's attempt has been interesting because it has attempted to collect data while human agents pretend artifical agents--basically, the wizard of OZ approach. Though the results of the attempt have not been publicly shared (?? this should be verified again), it would have been quite challenging because human users tend to change their behaviors depending on the quality of the service. In other words, as soon as users learned that the system is backed up by human agents and the responses from human agents are slow, people would issue more complicated and worthwhile-to-wait queries (e.g., what would be the best way to issue Canadian passport from Cuba?). Thus, the collected data from this approach would not work. Pretending to be artificial agents is quite challenging. In order to generate fast responses, you need to increase the human agents substantially and make them shifted to serve 24/7 services.

Only scalable ways to collect data for natural language understanding would be relying on text accumulated over the internet. There have been several approaches (e.g., DeepDive at Standford) to construct knowledge base using various statistical approaches. It is not fully automatic, but with some intervention from data curator, 

2. Still understanding natural languages

I partly agree with ??'s blog post regarding deep neural network is not perfectly aligned approach to learn human language. 

3. What's the most cognitive difficulties that human being have.

There are some tasks that human agents have more difficulties than others. If you review the mistakes that human agents made. Human factors is a good discipline to archieve various failures that human agents have made. Out of these issues, some solutions with the structured steps have been overcome by simply applying 

Follow the structure - 
Avoid stereotype - 
Getting tired - 
Knowing everythings - Open Q&A. 
Visilance - Reminder.

4. Infrastructure to help machine agents do its jobs better

5. Things should be still controlable

I don't believe that truly machine-learned experiences are not optimal. 

6. Focus on the most important things.

Really working natural language understanding. Some good source of data is necessary.

Unsupervised leanring